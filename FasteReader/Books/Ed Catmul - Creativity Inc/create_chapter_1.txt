For thirteen years we had a table in the large conference room at Pixar that we call West One. Though it was beautiful, I grew to hate this table. It was long and skinny, like one of those things you’d see in a comedy sketch about an old wealthy couple that sits down for dinner—one person at either end, a candelabra in the middle—and has to shout to make conversation. The table had been chosen by a designer Steve Jobs liked, and it was elegant, all right—but it impeded our work.

We’d hold regular meetings about our movies around that table—thirty of us facing off in two long lines, often with more people seated along the walls—and everyone was so spread out that it was difficult to communicate. For those unlucky enough to be seated at the far ends, ideas didn’t flow because it was nearly impossible to make eye contact without craning your neck. Moreover, because it was important that the director and producer of the film in question be able to hear what everyone was saying, they had to be placed at the center of the table. So did Pixar’s creative leaders: John Lasseter, Pixar’s creative officer, and me, and a handful of our most experienced directors, producers, and writers. To ensure that these people were always seated together, someone began making place cards. We might as well have been at a formal dinner party.

When it comes to creative inspiration, job titles and hierarchy are meaningless. That’s what I believe. But unwittingly, we were allowing this table—and the resulting place card ritual—to send a different message. The closer you were seated to the middle of the table, it implied, the more important—the more central—you must be. And the farther away, the less likely you were to speak up—your distance from the heart of the conversation made participating feel intrusive. If the table was crowded, as it often was, still more people would sit in chairs around the edges of the room, creating yet a third tier of participants (those at the center of the table, those at the ends, and those not at the table at all). Without intending to, we’d created an obstacle that discouraged people from jumping in.

Over the course of a decade, we held countless meetings around this table in this way—completely unaware of how doing so undermined our own core principles. Why were we blind to this? Because the seating arrangements and place cards were designed for the convenience of the leaders, including me. Sincerely believing that we were in an inclusive meeting, we saw nothing amiss because we didn’t feel excluded. Those not sitting at the center of the table, meanwhile, saw quite clearly how it established a pecking order but presumed that we—the leaders—had intended that outcome. Who were they, then, to complain?

It wasn’t until we happened to have a meeting in a smaller room with a square table that John and I realized what was wrong. Sitting around that table, the interplay was better, the exchange of ideas more free-flowing, the eye contact automatic. Every person there, no matter their job title, felt free to speak up. This was not only what we wanted, it was a fundamental Pixar belief: Unhindered communication was key, no matter what your position. At our long, skinny table, comfortable in our middle seats, we had utterly failed to recognize that we were behaving contrary to that basic tenet. Over time, we’d fallen into a trap. Even though we were conscious that a room’s dynamics are critical to any good discussion, even though we believed that we were constantly on the lookout for problems, our vantage point blinded us to what was right before our eyes.

Emboldened by this new insight, I went to our facilities department. “Please,” I said, “I don’t care how you do it, but get that table out of there.” I wanted something that could be arranged into a more intimate square, so people could address each other directly and not feel like they didn’t matter. A few days later, as a critical meeting on an upcoming movie approached, our new table was installed, solving the problem.

Still, interestingly, there were remnants of that problem that did not immediately vanish just because we’d solved it. For example, the next time I walked into West One, I saw the brand-new table, arranged—as requested—in a more intimate square that made it possible for more people to interact at once. But the table was adorned with the same old place cards! While we’d fixed the key problem that had made place cards seem necessary, the cards themselves had become a tradition that would continue until we specifically dismantled it. This wasn’t as troubling an issue as the table itself, but it was something we had to address because cards implied hierarchy, and that was precisely what we were trying to avoid. When Andrew Stanton, one of our directors, entered the meeting room that morning, he grabbed several place cards and began randomly moving them around, narrating as he went. “We don’t need these anymore!” he said in a way that everyone in the room grasped. Only then did we succeed in eliminating this ancillary problem.

This is the nature of management. Decisions are made, usually for good reasons, which in turn prompt other decisions. So when problems arise—and they always do—disentangling them is not as simple as correcting the original error. Often, finding a solution is a multi-step endeavor. There is the problem you know you are trying to solve—think of that as an oak tree—and then there are all the other problems—think of these as saplings—that sprouted from the acorns that fell around it. And these problems remain after you cut the oak tree down.

Even after all these years, I’m often surprised to find problems that have existed right in front of me, in plain sight. For me, the key to solving these problems is finding ways to see what’s working and what isn’t, which sounds a lot simpler than it is. Pixar today is managed according to this principle, but in a way I’ve been searching all my life for better ways of seeing. It began decades before Pixar even existed.


When I was a kid, I used to plunk myself down on the living room floor of my family’s modest Salt Lake City home a few minutes before 7 P.M. every Sunday and wait for Walt Disney. Specifically, I’d wait for him to appear on our black-and-white RCA with its tiny 12-inch screen. Even from a dozen feet away—the accepted wisdom at the time was that viewers should put one foot between them and the TV for every inch of screen—I was transfixed by what I saw.

Each week, Walt Disney himself opened the broadcast of The Wonderful World of Disney. Standing before me in suit and tie, like a kindly neighbor, he would demystify the Disney magic. He’d explain the use of synchronized sound in Steamboat Willie or talk about the importance of music in Fantasia. He always went out of his way to give credit to his forebears, the men—and, at this point, they were all men—who’d done the pioneering work upon which he was building his empire. He’d introduce the television audience to trailblazers such as Max Fleischer, of Koko the Clown and Betty Boop fame, and Winsor McCay, who made Gertie the Dinosaur—the first animated film to feature a character that expressed emotion—in 1914. He’d gather a group of his animators, colorists, and storyboard artists to explain how they made Mickey Mouse and Donald Duck come to life. Each week, Disney created a made-up world, used cutting-edge technology to enable it, and then told us how he’d done it.

Walt Disney was one of my two boyhood idols. The other was Albert Einstein. To me, even at a young age, they represented the two poles of creativity. Disney was all about inventing the new. He brought things into being—both artistically and technologically—that did not exist before. Einstein, by contrast, was a master of explaining that which already was. I read every Einstein biography I could get my hands on as well as a little book he wrote on his theory of relativity. I loved how the concepts he developed forced people to change their approach to physics and matter, to view the universe from a different perspective. Wild-haired and iconic, Einstein dared to bend the implications of what we thought we knew. He solved the biggest puzzles of all and, in doing so, changed our understanding of reality.

Both Einstein and Disney inspired me, but Disney affected me more because of his weekly visits to my family’s living room. “When you wish upon a star, makes no difference who you are,” his TV show’s theme song would announce as a baritone-voiced narrator promised: “Each week, as you enter this timeless land, one of these many worlds will open to you.… ” Then the narrator would tick them off: Frontierland (“tall tales and true from the legendary past”), Tomorrowland (“the promise of things to come”), Adventureland (“the wonder world of nature’s own realm”), and Fantasyland (“the happiest kingdom of them all”). I loved the idea that animation could take me places I’d never been. But the land I most wanted to learn about was the one occupied by the innovators at Disney who made these animated films.

Between 1950 and 1955, Disney made three movies we consider classics today: Cinderella, Peter Pan, and Lady and the Tramp. More than half a century later, we all remember the glass slipper, the Island of Lost Boys, and that scene where the cocker spaniel and the mutt slurp spaghetti. But few grasp how technically sophisticated these movies were. Disney’s animators were at the forefront of applied technology; instead of merely using existing methods, they were inventing ones of their own. They had to develop the tools to perfect sound and color, to use blue screen matting and multi-plane cameras and xerography. Every time some technological breakthrough occurred, Walt Disney incorporated it and then talked about it on his show in a way that highlighted the relationship between technology and art. I was too young to realize such a synergy was groundbreaking. To me, it just made sense that they belonged together.

Watching Disney one Sunday evening in April of 1956, I experienced something that would define my professional life. What exactly it was is difficult to describe except to say that I felt something fall into place inside my head. That night’s episode was called “Where Do the Stories Come From?” and Disney kicked it off by praising his animators’ knack for turning everyday occurrences into cartoons. That night, though, it wasn’t Disney’s explanation that pulled me in but what was happening on the screen as he spoke. An artist was drawing Donald Duck, giving him a jaunty costume and a bouquet of flowers and a box of candy with which to woo Daisy. Then, as the artist’s pencil moved around the page, Donald came to life, putting up his dukes to square off with the pencil lead, then raising his chin to allow the artist to give him a bow tie.

The definition of superb animation is that each character on the screen makes you believe it is a thinking being. Whether it’s a T-Rex or a slinky dog or a desk lamp, if viewers sense not just movement but intention—or, put another way, emotion—then the animator has done his or her job. It’s not just lines on paper anymore; it’s a living, feeling entity. This is what I experienced that night, for the first time, as I watched Donald leap off the page. The transformation from a static line drawing to a fully dimensional, animated image was sleight of hand, nothing more, but the mystery of how it was done—not just the technical process but the way the art was imbued with such emotion—was the most interesting problem I’d ever considered. I wanted to climb through the TV screen and be part of this world.


The mid-1950s and early 1960s were, of course, a time of great prosperity and industry in the United States. Growing up in Utah in a tight-knit Mormon community, my four younger brothers and sisters and I felt that anything was possible. Because the adults we knew had all lived through the Depression, World War II, and then the Korean War, this period felt to them like the calm after a thunderstorm.

I remember the optimistic energy—an eagerness to move forward that was enabled and supported by a wealth of emerging technologies. It was boom time in America, with manufacturing and home construction at an all-time high. Banks were offering loans and credit, which meant more and more people could own a new TV, house, or Cadillac. There were amazing new appliances like disposals that ate your garbage and machines that washed your dishes, although I certainly did my share of cleaning them by hand. The first organ transplants were performed in 1954; the first polio vaccine came a year later; in 1956, the term artificial intelligence entered the lexicon. The future, it seemed, was already here.

Then, when I was twelve, the Soviets launched the first artificial satellite—Sputnik 1—into earth’s orbit. This was huge news, not just in the scientific and political realms but in my sixth grade classroom at school, where the morning routine was interrupted by a visit from the principal, whose grim expression told us that our lives had changed forever. Since we’d been taught that the Communists were the enemy and that nuclear war could be waged at the touch of a button, the fact that they’d beaten us into space seemed pretty scary—proof that they had the upper hand.

The United States government’s response to being bested was to create something called ARPA, or the Advanced Research Projects Agency. Though it was housed within the Defense Department, its mission was ostensibly peaceful: to support scientific researchers in America’s universities in the hopes of preventing what it termed “technological surprise.” By sponsoring our best minds, the architects of ARPA believed, we’d come up with better answers. Looking back, I still admire that enlightened reaction to a serious threat: We’ll just have to get smarter. ARPA would have a profound effect on America, leading directly to the computer revolution and the Internet, among countless other innovations. There was a sense that big things were happening in America, with much more to come. Life was full of possibility.

Still, while my family was middle-class, our outlook was shaped by my father’s upbringing. Not that he talked about it much. Earl Catmull, the son of an Idaho dirt farmer, was one of fourteen kids, five of whom had died as infants. His mother, raised by Mormon pioneers who made a meager living panning for gold in the Snake River in Idaho, didn’t attend school until she was eleven. My father was the first in his family ever to go to college, paying his own way by working several jobs. During my childhood, he taught math during the school year and built houses during the summers. He built our house from the ground up. While he never explicitly said that education was paramount, my siblings and I all knew we were expected to study hard and go to college.

I was a quiet, focused student in high school. An art teacher once told my parents I would often become so lost in my work that I wouldn’t hear the bell ring at the end of class; I’d be sitting there, at my desk, staring at an object—a vase, say, or a chair. Something about the act of committing that object to paper was completely engrossing—the way it necessitated seeing only what was there and shutting out the distraction of my ideas about chairs or vases and what they were supposed to look like. At home, I sent away for Jon Gnagy’s Learn to Draw art kits—which were advertised in the back of comic books—and the 1948 classic Animation, written and drawn by Preston Blair, the animator of the dancing hippos in Disney’s Fantasia. I bought a platen—the flat metal plate artists use to press paper against ink—and even built a plywood animation stand with a light under it. I made flipbooks—one was of a man whose legs turned into a unicycle—while nursing my first crush, Tinker Bell, who had won my heart in Peter Pan.

Nevertheless, it soon became clear to me that I would never be talented enough to join Disney Animation’s vaunted ranks. What’s more, I had no idea how one actually became an animator. There was no school for it that I knew of. As I finished high school, I realized I had a far better understanding of how one became a scientist. The route seemed easier to discern. Throughout my life, people have always smiled when I told them I switched from art to physics because it seems, to them, like such an incongruous leap. But my decision to pursue physics, and not art, would lead me, indirectly, to my true calling.


Four years later, in 1969, I graduated from the University of Utah with two degrees, one in physics and the other in the emerging field of computer science. Applying to graduate school, my intention was to learn how to design computer languages. But soon after I matriculated, also at the U of U, I met a man who would encourage me to change course: one of the pioneers of interactive computer graphics, Ivan Sutherland.

The field of computer graphics—in essence, the making of digital pictures out of numbers, or data, that can be manipulated by a machine—was in its infancy then, but Professor Sutherland was already a legend. Early in his career, he had devised something called Sketchpad, an ingenious computer program that allowed figures to be drawn, copied, moved, rotated, or resized, all while retaining their basic properties. In 1968, he’d co-created what is widely believed to be the first virtual reality head-mounted display system. (The device was named The Sword of Damocles, after the Greek myth, because it was so heavy that in order to be worn by the person using it, it had to be suspended from a mechanical arm bolted to the ceiling.) Sutherland and Dave Evans, who was chair of the university’s computer science department, were magnets for bright students with diverse interests, and they led us with a light touch. Basically, they welcomed us to the program, gave us workspace and access to computers, and then let us pursue whatever turned us on. The result was a collaborative, supportive community so inspiring that I would later seek to replicate it at Pixar.

One of my classmates, Jim Clark, would go on to found Silicon Graphics and Netscape. Another, John Warnock, would co-found Adobe, known for Photoshop and the PDF file format, among other things. Still another, Alan Kay, would lead on a number of fronts, from object-oriented programming to “windowing” graphical user interfaces. In many respects, my fellow students were the most inspirational part of my university experience; this collegial, collaborative atmosphere was vital not just to my enjoyment of the program but also to the quality of the work that I did.

This tension between the individual’s personal creative contribution and the leverage of the group is a dynamic that exists in all creative environments, but this would be my first taste of it. On one end of the spectrum, I noticed, we had the genius who seemed to do amazing work on his or her own; on the other end, we had the group that excelled precisely because of its multiplicity of views. How, then, should we balance these two extremes, I wondered. I didn’t yet have a good mental model that would help me answer that, but I was developing a fierce desire to find one.

Much of the research being done at the U of U’s computer science department was funded by ARPA. As I’ve said, ARPA had been created in response to Sputnik, and one of its key organizing principles was that collaboration could lead to excellence. In fact, one of ARPA’s proudest achievements was linking universities with something they called “ARPANET,” which would eventually evolve into the Internet. The first four nodes on the ARPANET were at the Stanford Research Institute, UCLA, UC Santa Barbara, and the U of U, so I had a ringside seat from which to observe this grand experiment, and what I saw influenced me profoundly. ARPA’s mandate—to support smart people in a variety of areas—was carried out based on the unwavering presumption that researchers would try to do the right thing and, in ARPA’s view, overmanaging them was counterproductive. ARPA’s administrators did not hover over the shoulders of those of us working on the projects they funded, nor did they demand that our work have direct military applications. They simply trusted us to innovate.

This kind of trust gave me the freedom to tackle all sorts of complex problems, and I did so with gusto. Not only did I often sleep on the floor of the computer rooms to maximize time on the computer, but so did many of my fellow graduate students. We were young, driven by the sense that we were inventing the field from scratch—and that was exciting beyond words. For the first time, I saw a way to simultaneously create art and develop a technical understanding of how to create a new kind of imagery. Making pictures with a computer spoke to both sides of my brain. To be sure, the pictures that could be rendered on a computer were very crude in 1969, but the act of inventing new algorithms and seeing better pictures as a result was thrilling to me. In its own way, my childhood dream was reasserting itself.

At the age of twenty-six, I set a new goal: to develop a way to animate, not with a pencil but with a computer, and to make the images compelling and beautiful enough to use in the movies. Perhaps, I thought, I could become an animator after all.


In the spring of 1972, I spent ten weeks making my first short animated film—a digitized model of my left hand. My process combined old and new; again, like everyone in this fast-changing field, I was helping to invent the language. First I plunged my hand into a tub of plaster of Paris (forgetting, unfortunately, to coat it in Vaseline first, which meant I had to yank out every tiny hair on the back of my hand to get it free); then, once I had the mold, I filled it with more plaster to make a model of my hand; then, I took that model and covered it with 350 tiny interlocking triangles and polygons to create what looked like a net of black lines on its “skin.” You may not think that a curved surface could be built out of such flat, angular elements, but when you make them small enough, you can get pretty close.



I’d chosen this project because I was interested in rendering complex objects and curved surfaces—and I was looking for a challenge. At that time, computers weren’t great at showing flat objects, let alone curved ones. The mathematics of curved surfaces was not well developed, and computers had limited memory capability. At the U of U’s computer graphics department, where every one of us yearned to make computer-generated images look as if they were photographs of real objects, we had three driving goals: speed, realism, and the ability to depict curved surfaces. My film sought to address the latter two.

The human hand doesn’t have a single flat plane. And unlike a simpler curved surface—a ball, for example—it has many parts that act in opposition to one another, with a seemingly infinite number of resulting movements. The hand is an incredibly complex “object” to try to capture and translate into arrays of numbers. Given that most computer animation at the time consisted of rendering simple polygonal objects (cubes, pyramids), I had my work cut out for me.

Once I had drawn the triangles and polygons on my model, I measured the coordinates of each of their corners, then entered that data into a 3D animation program I’d written. That enabled me to display the many triangles and polygons that made up my virtual hand on a monitor. In its first incarnation, sharp edges could be seen at the seams where the polygons joined together. But later, thanks to “smooth shading”—a technique, developed by another graduate student, that diminished the appearance of those edges—the hand became more lifelike. The real challenge, though, was making it move.



Hand, which debuted at a computer science conference in 1973, caused a bit of a stir because no one had ever seen anything like it before. In it, my hand, which appears at first to be covered in a white net of polygons, begins to open and close, as if trying to make a fist. Then my hand’s surface becomes smoother, more like the real thing. There is a moment when my hand points directly at the viewer as if to say, “Yes, I’m talking to you.” Then, the camera goes inside the hand and takes a look around, aiming its lens inside the palm and up into each finger, a tricky bit of perspective that I liked because it could be depicted only via computer. Those four minutes of film had taken me more than sixty thousand minutes to complete.

Together with a digitized film that my friend Fred Parke made of his wife’s face around the same time, Hand represented the state-of-the-art in computer animation for years after it was made. Snippets of both Fred’s and my films would be featured in the 1976 movie Futureworld, which—though mostly forgotten by moviegoers today—is still remembered by aficionados as the first full-length feature to use computer-generated animation.


Professor Sutherland used to say that he loved his graduate students at Utah because we didn’t know what was impossible. Neither, apparently, did he: He was among the first to believe that Hollywood movie execs would care a fig about what was happening in academia. To that end, he sought to create a formal exchange program with Disney, wherein the studio would send one of its animators to Utah to learn about new technologies in computer rendering, and the university would send a student to Disney Animation to learn more about how to tell stories.

In the spring of 1973, he sent me to Burbank to try to sell this idea to the Disney executives. It was a thrill for me to drive through the red brick gates and onto the Disney lot on my way to the original Animation Building, built in 1940 with a “Double H” floor plan personally supervised by Walt himself to ensure that as many rooms as possible had windows to let in natural light. While I’d studied this place—or what I could glimpse of it on our 12-inch RCA—walking into it was a little like stepping into the Parthenon for the first time. There, I met Frank Thomas and Ollie Johnston, two of Walt’s “Nine Old Men,” the group of legendary animators who had created so many of the characters in the Disney movies I loved, from Pinocchio to Peter Pan. At one point I was taken into the archives where all the original paper drawings from all the animated films were kept, with rack after rack after rack of the images that had fueled my imagination. I’d entered the Promised Land.

One thing was immediately clear. The people I met at Disney—one of whom, I swear, was named Donald Duckwall—had zero interest in Sutherland’s exchange program. The technically adventuresome Walt Disney was long gone. My enthusiastic descriptions were met with blank stares. To them, computers and animation simply didn’t mix. How did they know this? Because the one time they had turned to computers for help—to render images of millions of bubbles in their 1971 live-action movie Bedknobs and Broomsticks—the computers had apparently let them down. The state of the technology at the time was so poor, particularly for curved images, that bubbles were beyond the computers’ reach. Unfortunately, this didn’t help my cause. “Well,” more than one Disney executive told me that day, “until computer animation can do bubbles, then it will not have arrived.”

Instead, they tried to tempt me into taking a job with what is now called Disney Imagineering, the division that designs the theme parks. It may sound odd, given how large Walt Disney had always loomed in my life, but I turned the offer down without hesitation. The theme park job felt like a diversion that would lead me down a path I didn’t want to be on. I didn’t want to design rides for a living. I wanted to animate with a computer.



Just as Walt Disney and the pioneers of hand-drawn animation had done decades before, those of us who sought to make pictures with computers were trying to create something new. When one of my colleagues at the U of U invented something, the rest of us would immediately piggyback on it, pushing that new idea forward. There were setbacks, too, of course. But the overriding feeling was one of progress, of moving steadily toward a distant goal.

Long before I’d heard about Disney’s bubble problem, what kept me and many of my fellow graduate students up at night was the need to continue to hone our methods for creating smoothly curved surfaces with the computer—as well as to figure out how to add richness and complexity to the images we were creating. My dissertation, “A Subdivision Algorithm for Computer Display of Curved Surfaces,” offered a solution to that problem.

Much of what I spent every waking moment thinking about then was extremely technical and difficult to explain, but I’ll give it a try. The idea behind what I called “subdivision surfaces” was that instead of setting out to depict the whole surface of a shiny, red bottle, for example, we could divide that surface into many smaller pieces. It was easier to figure out how to color and display each tiny piece—which we could then put together to create our shiny, red bottle. (As I’ve noted, computer memory capacity was quite small in those days, so we put a lot of energy into developing tricks to overcome that limitation. This was one of those tricks.) But what if you wanted that shiny, red bottle to be zebra-striped? In my dissertation, I figured out a way that I could take a zebra-print or woodgrain pattern, say, and wrap it around any object.

“Texture mapping,” as I called it, was like having stretchable wrapping paper that you could apply to a curved surface so that it fit snugly. The first texture map I made involved projecting an image of Mickey Mouse onto an undulating surface.

I also used Winnie the Pooh and Tigger to illustrate my points. I may not have been ready to work at Disney, but their characters were still the touchstones I referenced.

At the U of U, we were inventing a new language. One of us would contribute a verb, another a noun, then a third person would figure out ways to string the elements together to actually say something. My invention of something called the “Z-buffer” was a good example of this, in that it built on others’ work. The Z-buffer was designed to address the problem of what happens when one computer-animated object is hidden, or partially hidden, behind another one. Even though the data that describes every aspect of the hidden object is in the computer’s memory (meaning that you could see it, if need be), the desired spatial relationships mean that it should not be fully seen. The challenge was to figure out a way to tell the computer to meet that goal. For example, if a sphere were in front of a cube, partially blocking it, the sphere’s surface should be visible on the screen, as should the parts of the cube that are not blocked by the sphere. The Z-buffer accomplished that by assigning a depth to every object in three-dimensional space, then telling the computer to match each of the screen’s pixels to whatever object was the closest. Computer memory was so limited—as I’ve said—that this wasn’t a practical solution, but I had found a new way of solving the problem. Although it sounds simple, it is anything but. Today, there is a Z-buffer in every game and PC chip manufactured on earth.

After receiving my Ph.D. in 1974, I left Utah with a nice little list of innovations under my belt, but I was keenly aware that I’d only done all this in the service of a larger mutual goal. Like my classmates, the work I’d championed had taken hold largely because of the protective, eclectic, intensely challenging environment I’d been in. The leaders of my department understood that to create a fertile laboratory, they had to assemble different kinds of thinkers and then encourage their autonomy. They had to offer feedback when needed but also had to be willing to stand back and give us room. I felt instinctively that this kind of environment was rare and worth reaching for. I knew that the most valuable thing I was taking away from the U of U was the model my teachers had provided for how to lead and inspire other creative thinkers. The question for me, then, was how to get myself into another environment like this—or how to build one of my own.

I walked away from Utah with a clearer sense of my goal, and I was prepared to devote my life to it: making the first computer-animated film. But getting to that point would not be easy. There were, I guessed, at least another ten years of development needed to figure out how to model and animate characters and render them in complex environments before we could even begin to conceive of making a short—let alone a feature—film. I also didn’t yet know that my self-assigned mission was about much more than technology. To pull it off, we’d have to be creative not only technically but also in the ways that we worked together.

Back then, no other company or university shared my goal of making a computer-generated film; in fact, each time I expressed that goal in job interviews at universities, it seemed to cast a pall over the room. “But we want you to teach computer science,” my interviewers would say. What I was proposing to do looked, to most academics, like a pipe dream, an expensive fantasy.

Then, in November 1974, I received a mysterious call from a woman who said she worked at something called the New York Institute of Technology. She said she was the secretary to the institute’s president, and she was calling to book my airplane ticket. I didn’t know what she was talking about, and I told her so. What was the name of the institute again? I asked. Why did she want me to fly to New York? There was an awkward silence. “I’m sorry,” she said. “Someone else was supposed to call you before I did.”

And with that, she hung up. The next phone call I received would change my life.